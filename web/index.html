<!DOCTYPE html>
<html>
  <head>
    <title>a_bowl_of_grain</title>
    <!-- Link to external CSS file -->
    <link rel="stylesheet" type="text/css" href="styles.css" />
  </head>

  <body>
    <script src="text_animation.js"></script>
    <nav class="flex row">
      <h2>a_bowl_of_grain</h2>

      <a href="https://github.com/ruarim/a_bowl_of_grain" target="”_blank”"
        >See the code</a
      >
    </nav>

    <div class="bar"></div>

    <div id="hero" class="flex row">
      <h1>
        AN INTERFACE<br />FOR<br />
        <div class="granular">
          <span class="key">G</span>
          <span class="key">R</span>
          <span class="key">A</span>
          <span class="key">N</span>
          <span class="key">U</span>
          <span class="key">L</span>
          <span class="key">A</span>
          <span class="key">R</span>
        </div>
        SYNTHESIS.
      </h1>
    </div>

    <div id="part3" class="flex column">
      <iframe
        width="1400"
        height="700"
        src="https://www.youtube.com/embed/I2vvDrNJBnQ"
      >
      </iframe>
    </div>

    <div id="part1">
      <div class="part1-info flex column">
        <h2>CONTEXT, CONCEPT, AIMS</h2>
      </div>
      <div>
        <div class="flex row part1-content" style="gap: 3px">
          <p style="padding: 1px">
            The main goal of this project was to create an intuitive interface
            for granular synthesis. This synthesis technique is fundamentally
            about manipulating and processing tiny pieces of sound to create new
            textures and timbres. Curtis Roads, a key figure in the development
            of granular synthesis, offers a concise definition of this technique
            in his 1988 paper "An Introduction to Granular Synthesis." He
            states, "Granular synthesis involves generating thousands of very
            short sonic grains to form larger acoustic events." This definition
            encapsulates the essence of granular synthesis, where many tiny,
            brief sound elements-grains-are produced and combined.
          </p>
          <p style="padding: 1px">
            The piano keyboard, which is commonly used to interface with
            granular synthesis, does not complement the technique and provides
            limited control. This is mainly due to the large number of
            parameters available and the limited reliance on pitch. Therefore,
            interfaces that offer multi-dimensional control allow for a more
            nuanced and dynamic manipulation of the complex parameters involved
            in granular synthesis. With this in mind, a_bowl_of_grain aims to
            provide a tactile multidimensional connection to the grains of sound
            created by granular synthesis. Digital musical instruments (DMI)
            such as the GrainPlain, MetaMuse, PebbleBox and CrumbleBag tackle
            this idea. Each of them provides a slightly different take on this
            tactile connection.
          </p>
        </div>
        <div id="image-container" class="flex column">
          <img style="width: 600px" src="images/inspiration.png" />
          <p>Projects which inspired the instrument.</p>
        </div>
      </div>
    </div>

    <div id="part1">
      <div class="part1-info flex column">
        <h2>DESIGN PROCESS, TECHNICAL AND CREATIVE DETAILS</h2>
      </div>
      <div class="part1-content" style="gap: 10px">
        <p>
          My initial ideas centred around detecting when an object created
          vibration on the surface of a resonant material and then using these
          vibrations to trigger grains of sound. Also, I wanted to provide
          multi-dimensional control of the grain via localising the vibration on
          the surface. This idea was inspired by the GrainPlain, which maps the
          x and y location of vibrations on a flat metal surface to granular
          synthesiser parameters.
        </p>
        <div id="image-container" class="flex column">
          <img style="width: 600px" src="images/grain_detection.png" />
          <p>Grain detection diagram.</p>
        </div>
        <br />
        <p>
          A metal bowl with attached contact mics was decided upon as an
          appropriate system for detecting vibrations. As well as vibrations
          travelling well through the metal surface, the bowl shape provides
          control over the objects, allowing them to be ‘tossed around’ in a
          more rhythmic and musical way without the player touching the surface.
          Additionally, the bowl is a well-known vessel for objects, so it is
          intuitive as to how the grains can be manipulated. This idea is
          inspired by Perry Cook's principle, “Household objects make amusing
          controllers”, from the 2001 paper Principles for Designing Computer
          Music Controllers. Also, since the bowl will be tilted and shaken to
          move the objects around, it seemed appropriate to use an accelerometer
          to add extra control dimensions.
        </p>
        <div id="image-container" class="flex column">
          <img style="width: 600px" src="images/mappings.png" />
          <p>Sensor mappings diagram.</p>
        </div>
        <br />
        <p style="font-weight: bold; font-size: 30px">
          The system is comprised of four parts.
        </p>
        <br />
        <p style="font-weight: bold">The Sensors</p>
        <p>
          <br />
          Four contact mics to detect vibrations, and an accelerometer to detect
          tilt were wired together and attached to the bottom of the bowl.
        </p>

        <div id="image-container" class="flex column">
          <img style="width: 600px" src="images/circuit_diagram.png" />
          <p>Circuit schematic.</p>
        </div>
        <div id="image-container" class="flex column">
          <img style="width: 400px" src="images/final_wiring.jpg" />
          <p>Sensors wiring.</p>
        </div>

        <br />
        <p style="font-weight: bold">The Arduino</p>
        <p>
          <br />
          Values from the sensors are read into the Arduino via the analogue
          pins and sampled at a set interval. The contact mic values are
          processed, and the closest mic to the vibration is found. The data
          from the accelerometer is processed by the
          <span
            ><a href="https://github.com/RCmags/basicMPU6050" target="_blank"
              >basicMPU6050</a
            ></span
          >
          library. The amplitude level, microphone index and accelerometer x, y
          values are then output to the serial bus.
          <br />
        </p>
        <div id="image-container" class="flex column">
          <img style="width: 500px" src="images/main_arudino_code.png" />
          <p>Arduino code main loop function.</p>
        </div>
        <br />
        <p style="font-weight: bold">The Granular synthesiser</p>
        <p>
          <br />
          A Max patch running a granular synthesiser implemented with the
          grainflow package is then used to read values from the serial port. A
          vibration detected from the contact mic triggers the granular synth to
          create a grain. The amplitude of the vibration controls the loudness
          of the grain. The x value from the accelerometer controls at what
          point a grain should be created in the original sample source. This
          allows exploration of the original sample via tilting the bowl left or
          right. The y value from the accelerometer allows a lowpass filter,
          applied to the grains, to be opened or closed by tilting the bowl
          forward or back.
          <br />
          <br />
        </p>
        <div id="image-container" class="flex column">
          <img style="width: 500px" src="images/granular_synth.png" />
          <p>Granular synth Max code.</p>
        </div>
        <p style="font-weight: bold">Spatalisation</p>
        <p>
          <br />
          In my instrument performance, a 24-channel ambisonics setup was used
          to spatialise the grains. Throughout the piece, 4 speakers were
          constantly playing grains. Depending on which contact mic was closest
          to the vibration, another 5 speakers played grains. The 5 speakers
          were grouped into front right, front left, back right, and back left.
          This was implemented in the same Max patch using the MC library.
        </p>
        <div id="image-container" class="flex column">
          <img style="width: 600px" src="images/spatalisation.png" />
          <p>Spatalisation mapping and design.</p>
        </div>
      </div>
    </div>

    <div id="part1">
      <div class="part1-info flex column">
        <h2>CRITICAL ANALYSIS</h2>
      </div>
      <div class="part1-content">
        <p>
          Overall, the outcome met the main goal of my design, which was to
          create a direct physical connection to synthesised grains of sound.
          The bowl form supports this idea nicely and provides intuitive control
          over the grains via tilting the accelerometer. After showing the
          instrument to friends, most agreed it was simple to pick up and use
          without understanding of the Granular synthesis method.
        </p>
        <br />
        <br />
        <p>
          Initially, I wanted to pinpoint the exact location of vibrations on
          the bowl, although after reading many Arduino forum posts, this turned
          out to be a very difficult task. This was primarily due to the bowl's
          small size and rounded shape. To tackle this, I split the bowl into
          quarters and localised the vibration to one of the areas in the bowl.
        </p>
        <br />
        <br />
        <p>
          The gestures provided by the accelerometer worked well, although they
          were relatively static. I would have liked to implement more complex
          gestures, such as flicks and twists, that could provide more dynamic
          control over the instrument's timbre. Making the instrument wireless
          via a Wi-Fi-enabled chip and some Open Sound Control messages could
          also enhance its gestural capabilities.
        </p>
        <br />
        <br />
        <p>
          While completing this project, I gained a much better understanding of
          how to wire up electronics. Setting up the sensors and reading their
          data sheets has given me a good fundamental understanding of their
          capabilities providing inspiration for future projects.
        </p>
        <br />
        <br />
        <p>
          The spatialisation techniques used were relatively primitive and
          implemented using Max's built-in MC objects. Although they get the job
          done, I would like to explore an ambisonics implementation in the
          future via ICRAMs
          <span
            ><a href="https://forum.ircam.fr/projects/detail/spat/"
              >spat software</a
            ></span
          >.
        </p>
        <div id="image-container" class="flex column">
          <img style="width: 400px" src="images/final_angleview.jpg" />
          <p>Final instrument hardware.</p>
        </div>
        <div id="image-container" class="flex column">
          <img style="width: 600px" src="images/max_ui.png" />
          <p>Final Max GUI.</p>
        </div>
        <br />
      </div>
    </div>

    <div id="part3" class="flex column">
      <a href="https://github.com/ruarim" target="_blank">By Ruari Molyneux</a>
    </div>
  </body>
</html>
